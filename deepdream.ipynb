{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_Diplomka.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kjVqd9Zhwd5q",
        "p8USDeZEvs-w",
        "SJvqTFF4Aehy",
        "eSWLAWDFLIz0"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBBCM_XPLIih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/fizyr/keras-retinanet.git\n",
        "%cd /content/keras-retinanet/\n",
        "!pip install .\n",
        "%cd /content/keras-retinanet/\n",
        "!python setup.py build_ext --inplace\n",
        "%cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnCQJE1iLOEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwD2je9-v5I7",
        "colab_type": "text"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYbsw0XhLU7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show images inline\n",
        "%matplotlib inline\n",
        "\n",
        "# automatically reload modules when they have changed\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# import keras\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.applications import resnet50\n",
        "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "from keras.applications import inception_v3\n",
        "from keras.utils import plot_model\n",
        "\n",
        "# import keras_retinanet\n",
        "from keras_retinanet import models\n",
        "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
        "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
        "from keras_retinanet.utils.colors import label_color\n",
        "from keras_retinanet.utils.gpu import setup_gpu\n",
        "from keras_retinanet.utils.compute_overlap import compute_overlap\n",
        "\n",
        "# import miscellaneous modules\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy\n",
        "import time\n",
        "import argparse\n",
        "from __future__ import print_function\n",
        "import urllib\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# cv2.imshow() causes jupyter colapse\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import tensorflow v1\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "\n",
        "# use this to change which GPU to use\n",
        "gpu = 0\n",
        "\n",
        "# set the modified tf session as backend in keras\n",
        "setup_gpu(gpu)\n",
        "\n",
        "drive_path = '/content/drive/' # set to your desired folder\n",
        "save_path = drive_path         # set to your desired folder\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ81knemvqB6",
        "colab_type": "text"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmF3q3CYcShT",
        "colab_type": "code",
        "outputId": "518fab18-3247-4fb9-b9ba-d5614cd49970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "### import model\n",
        "model_url = 'https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5'\n",
        "model_path = os.path.join('.', 'keras-retinanet', 'snapshots', 'resnet50_coco.h5')\n",
        "urllib.request.urlretrieve(model_url, model_path)\n",
        "\n",
        "# We will not be training our model,\n",
        "# so we use this command to disable all training-specific operations\n",
        "K.set_learning_phase(0)\n",
        "\n",
        "# load retinanet model\n",
        "model = models.load_model(model_path, backbone_name='resnet50')\n",
        "\n",
        "# load label to names mapping for visualization purposes\n",
        "labels_to_names = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
        "\n",
        "model_layers_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "\n",
        "fig_size = (10,10)\n",
        "mode = 'caffe'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "tracking <tf.Variable 'Variable:0' shape=(9, 4) dtype=float32> anchors\n",
            "tracking <tf.Variable 'Variable_1:0' shape=(9, 4) dtype=float32> anchors\n",
            "tracking <tf.Variable 'Variable_2:0' shape=(9, 4) dtype=float32> anchors\n",
            "tracking <tf.Variable 'Variable_3:0' shape=(9, 4) dtype=float32> anchors\n",
            "tracking <tf.Variable 'Variable_4:0' shape=(9, 4) dtype=float32> anchors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjVqd9Zhwd5q",
        "colab_type": "text"
      },
      "source": [
        "### defs: focal loss and compute targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9yogzkA1UyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=1\n",
        "num_classes = 80\n",
        "\n",
        "def focal(alpha=0.25, gamma=2.0):\n",
        "    \"\"\" Create a functor for computing the focal loss.\n",
        "\n",
        "    Args\n",
        "        alpha: Scale the focal weight with alpha.\n",
        "        gamma: Take the power of the focal weight with gamma.\n",
        "\n",
        "    Returns\n",
        "        A functor that computes the focal loss using the alpha and gamma.\n",
        "    \"\"\"\n",
        "    def _focal(y_true, y_pred):\n",
        "        \"\"\" Compute the focal loss given the target tensor and the predicted tensor.\n",
        "\n",
        "        As defined in https://arxiv.org/abs/1708.02002\n",
        "\n",
        "        Args\n",
        "            y_true: Tensor of target data from the generator with shape (B, N, num_classes).\n",
        "            y_pred: Tensor of predicted data from the network with shape (B, N, num_classes).\n",
        "\n",
        "        Returns\n",
        "            The focal loss of y_pred w.r.t. y_true.\n",
        "        \"\"\"\n",
        "        labels         = y_true[:, :, :-1]\n",
        "        anchor_state   = y_true[:, :, -1]  # -1 for ignore, 0 for background, 1 for object\n",
        "        classification  = y_pred\n",
        "\n",
        "        # filter out \"ignore\" anchors\n",
        "        indices        = tf.where(keras.backend.not_equal(anchor_state, -1))\n",
        "        labels         = tf.gather_nd(labels, indices)\n",
        "        classification  = tf.gather_nd(classification, indices)\n",
        "\n",
        "        # compute the focal loss\n",
        "        alpha_factor = keras.backend.ones_like(labels) * alpha\n",
        "        alpha_factor = tf.where(keras.backend.equal(labels, 1), alpha_factor, 1 - alpha_factor)\n",
        "        focal_weight = tf.where(keras.backend.equal(labels, 1), 1 - classification, classification)\n",
        "        focal_weight = alpha_factor * focal_weight ** gamma\n",
        "\n",
        "        cls_loss = focal_weight * keras.backend.binary_crossentropy(labels, classification)\n",
        "\n",
        "        # compute the normalizer: the number of positive anchors\n",
        "        normalizer = tf.where(keras.backend.equal(anchor_state, 1))\n",
        "        normalizer = keras.backend.cast(keras.backend.shape(normalizer)[0], keras.backend.floatx())\n",
        "        normalizer = keras.backend.maximum(keras.backend.cast_to_floatx(1.0), normalizer)\n",
        "\n",
        "        return keras.backend.sum(cls_loss) / normalizer\n",
        "\n",
        "    return _focal\n",
        "\n",
        "\n",
        "def smooth_l1(sigma=3.0):\n",
        "    \"\"\" Create a smooth L1 loss functor.\n",
        "\n",
        "    Args\n",
        "        sigma: This argument defines the point where the loss changes from L2 to L1.\n",
        "\n",
        "    Returns\n",
        "        A functor for computing the smooth L1 loss given target data and predicted data.\n",
        "    \"\"\"\n",
        "    sigma_squared = sigma ** 2\n",
        "\n",
        "    def _smooth_l1(y_true, y_pred):\n",
        "        \"\"\" Compute the smooth L1 loss of y_pred w.r.t. y_true.\n",
        "\n",
        "        Args\n",
        "            y_true: Tensor from the generator of shape (B, N, 5). The last value for each box is the state of the anchor (ignore, negative, positive).\n",
        "            y_pred: Tensor from the network of shape (B, N, 4).\n",
        "\n",
        "        Returns\n",
        "            The smooth L1 loss of y_pred w.r.t. y_true.\n",
        "        \"\"\"\n",
        "        # separate target and state\n",
        "        regression        = y_pred\n",
        "        regression_target = y_true[:, :, :-1]\n",
        "        anchor_state      = y_true[:, :, -1]\n",
        "\n",
        "        # filter out \"ignore\" anchors\n",
        "        indices           = tf.where(keras.backend.equal(anchor_state, 1))\n",
        "        regression        = tf.gather_nd(regression, indices)\n",
        "        regression_target = tf.gather_nd(regression_target, indices)\n",
        "\n",
        "        # compute smooth L1 loss\n",
        "        # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n",
        "        #        |x| - 0.5 / sigma / sigma    otherwise\n",
        "        regression_diff = regression - regression_target\n",
        "        regression_diff = keras.backend.abs(regression_diff)\n",
        "        regression_loss = tf.where(\n",
        "            keras.backend.less(regression_diff, 1.0 / sigma_squared),\n",
        "            0.5 * sigma_squared * keras.backend.pow(regression_diff, 2),\n",
        "            regression_diff - 0.5 / sigma_squared\n",
        "        )\n",
        "\n",
        "        # compute the normalizer: the number of positive anchors\n",
        "        normalizer = keras.backend.maximum(1, keras.backend.shape(indices)[0])\n",
        "        normalizer = keras.backend.cast(normalizer, dtype=keras.backend.floatx())\n",
        "        return keras.backend.sum(regression_loss) / normalizer\n",
        "\n",
        "    return _smooth_l1\n",
        "\n",
        "def preprocess_group_entry(image, annotations):\n",
        "    \"\"\" Preprocess image and its annotations.\n",
        "    \"\"\"\n",
        "    # preprocess the image\n",
        "    image = preprocess_image(image)\n",
        "\n",
        "    # convert to the wanted keras floatx\n",
        "    image = K.cast_to_floatx(image)\n",
        "\n",
        "    return image, annotations\n",
        "\n",
        "def preprocess_group(image_group, annotations_group):\n",
        "    \"\"\" Preprocess each image and its annotations in its group.\n",
        "    \"\"\"\n",
        "    assert(len(image_group) == len(annotations_group))\n",
        "\n",
        "    for index in range(len(image_group)):\n",
        "        # preprocess a single group entry\n",
        "        image_group[index], annotations_group[index] = preprocess_group_entry(image_group[index], annotations_group[index])\n",
        "\n",
        "    return image_group, annotations_group\n",
        "\n",
        "def compute_inputs(image_group):\n",
        "    \"\"\" Compute inputs for the network using an image_group.\n",
        "    \"\"\"\n",
        "    # get the max image shape\n",
        "    max_shape = tuple(max(image.shape[x] for image in image_group) for x in range(3))\n",
        "\n",
        "    # construct an image batch object\n",
        "    image_batch = np.zeros((batch_size,) + max_shape, dtype=K.floatx())\n",
        "\n",
        "    # copy all images to the upper left part of the image batch object\n",
        "    for image_index, image in enumerate(image_group):\n",
        "        image_batch[image_index, :image.shape[0], :image.shape[1], :image.shape[2]] = image\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        image_batch = image_batch.transpose((0, 3, 1, 2))\n",
        "\n",
        "    return image_batch\n",
        "\n",
        "def compute_targets(image_group, annotations_group):\n",
        "    \"\"\" Compute target outputs for the network using images and their annotations.\n",
        "    \"\"\"\n",
        "    # get the max image shape\n",
        "    max_shape = tuple(max(image.shape[x] for image in image_group) for x in range(3))\n",
        "    anchors   = anchors_for_shape(max_shape)\n",
        "\n",
        "    batches = anchor_targets_bbox(\n",
        "        anchors,\n",
        "        image_group,\n",
        "        annotations_group,\n",
        "        num_classes\n",
        "    )\n",
        "\n",
        "    return list(batches)\n",
        "\n",
        "class AnchorParameters:\n",
        "    \"\"\" The parameteres that define how anchors are generated.\n",
        "\n",
        "    Args\n",
        "        sizes   : List of sizes to use. Each size corresponds to one feature level.\n",
        "        strides : List of strides to use. Each stride correspond to one feature level.\n",
        "        ratios  : List of ratios to use per location in a feature map.\n",
        "        scales  : List of scales to use per location in a feature map.\n",
        "    \"\"\"\n",
        "    def __init__(self, sizes, strides, ratios, scales):\n",
        "        self.sizes   = sizes\n",
        "        self.strides = strides\n",
        "        self.ratios  = ratios\n",
        "        self.scales  = scales\n",
        "\n",
        "    def num_anchors(self):\n",
        "        return len(self.ratios) * len(self.scales)\n",
        "\"\"\"\n",
        "The default anchor parameters.\n",
        "\"\"\"\n",
        "AnchorParameters.default = AnchorParameters(\n",
        "    sizes   = [32, 64, 128, 256, 512],\n",
        "    strides = [8, 16, 32, 64, 128],\n",
        "    ratios  = np.array([0.5, 1, 2], keras.backend.floatx()),\n",
        "    scales  = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)], keras.backend.floatx()),\n",
        ")\n",
        "\n",
        "def anchors_for_shape(\n",
        "    image_shape,\n",
        "    pyramid_levels=None,\n",
        "    anchor_params=None,\n",
        "    shapes_callback=None,\n",
        "):\n",
        "    \"\"\" Generators anchors for a given shape.\n",
        "\n",
        "    Args\n",
        "        image_shape: The shape of the image.\n",
        "        pyramid_levels: List of ints representing which pyramids to use (defaults to [3, 4, 5, 6, 7]).\n",
        "        anchor_params: Struct containing anchor parameters. If None, default values are used.\n",
        "        shapes_callback: Function to call for getting the shape of the image at different pyramid levels.\n",
        "\n",
        "    Returns\n",
        "        np.array of shape (N, 4) containing the (x1, y1, x2, y2) coordinates for the anchors.\n",
        "    \"\"\"\n",
        "\n",
        "    if pyramid_levels is None:\n",
        "        pyramid_levels = [3, 4, 5, 6, 7]\n",
        "\n",
        "    if anchor_params is None:\n",
        "        anchor_params = AnchorParameters.default\n",
        "\n",
        "    if shapes_callback is None:\n",
        "        shapes_callback = guess_shapes\n",
        "    image_shapes = shapes_callback(image_shape, pyramid_levels)\n",
        "    print('Image shapes:', image_shapes)\n",
        "\n",
        "    # compute anchors over all pyramid levels\n",
        "    all_anchors = np.zeros((0, 4))\n",
        "    for idx, p in enumerate(pyramid_levels):\n",
        "        anchors = generate_anchor_windows(                                              \n",
        "            base_size=anchor_params.sizes[idx],\n",
        "            ratios=anchor_params.ratios,\n",
        "            scales=anchor_params.scales\n",
        "        )\n",
        "        shifted_anchors = shift(image_shapes[idx], anchor_params.strides[idx], anchors)  \n",
        "        all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
        "\n",
        "    return all_anchors\n",
        "\n",
        "def generate_anchor_windows(base_size=16, ratios=None, scales=None): \n",
        "    \"\"\"\n",
        "    Generate anchor (reference) windows by enumerating aspect ratios X\n",
        "    scales w.r.t. a reference window.\n",
        "    \"\"\"\n",
        "\n",
        "    if ratios is None:\n",
        "        ratios = AnchorParameters.default.ratios\n",
        "\n",
        "    if scales is None:\n",
        "        scales = AnchorParameters.default.scales\n",
        "\n",
        "    num_anchors = len(ratios) * len(scales)\n",
        "\n",
        "    # initialize output anchors\n",
        "    anchors = np.zeros((num_anchors, 4))\n",
        "\n",
        "    # scale base_size\n",
        "    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
        "\n",
        "    # compute areas of anchors\n",
        "    areas = anchors[:, 2] * anchors[:, 3]\n",
        "\n",
        "    # correct for ratios\n",
        "    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
        "    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
        "\n",
        "    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
        "    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
        "    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
        "\n",
        "    return anchors\n",
        "\n",
        "def shift(shape, stride, anchors):\n",
        "    \"\"\" Produce shifted anchors based on shape of the map and stride size.\n",
        "\n",
        "    Args\n",
        "        shape  : Shape to shift the anchors over.\n",
        "        stride : Stride to shift the anchors with over the shape.\n",
        "        anchors: The anchors to apply at each location.\n",
        "    \"\"\"\n",
        "\n",
        "    # create a grid starting from half stride from the top left corner\n",
        "    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
        "    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
        "\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "\n",
        "    shifts = np.vstack((\n",
        "        shift_x.ravel(), shift_y.ravel(),\n",
        "        shift_x.ravel(), shift_y.ravel()\n",
        "    )).transpose()\n",
        "\n",
        "    # add A anchors (1, A, 4) to\n",
        "    # cell K shifts (K, 1, 4) to get\n",
        "    # shift anchors (K, A, 4)\n",
        "    # reshape to (K*A, 4) shifted anchors\n",
        "    A = anchors.shape[0]\n",
        "    K = shifts.shape[0]\n",
        "    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n",
        "    all_anchors = all_anchors.reshape((K * A, 4))\n",
        "\n",
        "    return all_anchors\n",
        "\n",
        "\n",
        "def guess_shapes(image_shape, pyramid_levels):\n",
        "    \"\"\"Guess shapes based on pyramid levels.\n",
        "\n",
        "    Args\n",
        "         image_shape: The shape of the image.\n",
        "         pyramid_levels: A list of what pyramid levels are used.\n",
        "\n",
        "    Returns\n",
        "        A list of image shapes at each pyramid level.\n",
        "    \"\"\"\n",
        "    image_shape = np.array(image_shape[:2])\n",
        "    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n",
        "    return image_shapes\n",
        "\n",
        "\n",
        "\n",
        "def anchor_targets_bbox(\n",
        "    anchors,\n",
        "    image_group,\n",
        "    annotations_group,\n",
        "    num_classes,\n",
        "    negative_overlap=0.4,\n",
        "    positive_overlap=0.5\n",
        "):\n",
        "    \"\"\" Generate anchor targets for bbox detection.\n",
        "\n",
        "    Args\n",
        "        anchors: np.array of annotations of shape (N, 4) for (x1, y1, x2, y2).\n",
        "        image_group: List of BGR images.\n",
        "        annotations_group: List of annotations (np.array of shape (N, 5) for (x1, y1, x2, y2, label)).\n",
        "        num_classes: Number of classes to predict.\n",
        "        mask_shape: If the image is padded with zeros, mask_shape can be used to mark the relevant part of the image.\n",
        "        negative_overlap: IoU overlap for negative anchors (all anchors with overlap < negative_overlap are negative).\n",
        "        positive_overlap: IoU overlap or positive anchors (all anchors with overlap > positive_overlap are positive).\n",
        "\n",
        "    Returns\n",
        "        labels_batch: batch that contains labels & anchor states (np.array of shape (batch_size, N, num_classes + 1),\n",
        "                      where N is the number of anchors for an image and the last column defines the anchor state (-1 for ignore, 0 for bg, 1 for fg).\n",
        "        regression_batch: batch that contains bounding-box regression targets for an image & anchor states (np.array of shape (batch_size, N, 4 + 1),\n",
        "                      where N is the number of anchors for an image, the first 4 columns define regression targets for (x1, y1, x2, y2) and the\n",
        "                      last column defines anchor states (-1 for ignore, 0 for bg, 1 for fg).\n",
        "    \"\"\"\n",
        "\n",
        "    assert(len(image_group) == len(annotations_group)), \"The length of the images and annotations need to be equal.\"\n",
        "    assert(len(annotations_group) > 0), \"No data received to compute anchor targets for.\"\n",
        "    for annotations in annotations_group:\n",
        "        assert('bboxes' in annotations), \"Annotations should contain bboxes.\"\n",
        "        assert('labels' in annotations), \"Annotations should contain labels.\"\n",
        "\n",
        "    batch_size = len(image_group)\n",
        "\n",
        "    regression_batch  = np.zeros((batch_size, anchors.shape[0], 4 + 1), dtype=K.floatx())\n",
        "    labels_batch      = np.zeros((batch_size, anchors.shape[0], num_classes + 1), dtype=K.floatx())\n",
        "\n",
        "    # compute labels and regression targets\n",
        "    for index, (image, annotations) in enumerate(zip(image_group, annotations_group)):\n",
        "        if annotations['bboxes'].shape[0]:\n",
        "            # obtain indices of gt annotations with the greatest overlap\n",
        "            positive_indices, ignore_indices, argmax_overlaps_inds = compute_gt_annotations(anchors, annotations['bboxes'], negative_overlap, positive_overlap)\n",
        "\n",
        "            labels_batch[index, ignore_indices, -1]       = -1\n",
        "            labels_batch[index, positive_indices, -1]     = 1\n",
        "\n",
        "            regression_batch[index, ignore_indices, -1]   = -1\n",
        "            regression_batch[index, positive_indices, -1] = 1\n",
        "\n",
        "            # compute target class labels\n",
        "            labels_batch[index, positive_indices, annotations['labels'][argmax_overlaps_inds[positive_indices]].astype(int)] = 1\n",
        "\n",
        "            regression_batch[index, :, :-1] = bbox_transform(anchors, annotations['bboxes'][argmax_overlaps_inds, :])\n",
        "\n",
        "        # ignore annotations outside of image\n",
        "        if image.shape:\n",
        "            anchors_centers = np.vstack([(anchors[:, 0] + anchors[:, 2]) / 2, (anchors[:, 1] + anchors[:, 3]) / 2]).T\n",
        "            indices = np.logical_or(anchors_centers[:, 0] >= image.shape[1], anchors_centers[:, 1] >= image.shape[0])\n",
        "\n",
        "            labels_batch[index, indices, -1]     = -1\n",
        "            regression_batch[index, indices, -1] = -1\n",
        "\n",
        "    return regression_batch, labels_batch\n",
        "\n",
        "def compute_gt_annotations(\n",
        "    anchors,\n",
        "    annotations,\n",
        "    negative_overlap=0.4,\n",
        "    positive_overlap=0.5\n",
        "):\n",
        "    \"\"\" Obtain indices of gt annotations with the greatest overlap.\n",
        "\n",
        "    Args\n",
        "        anchors: np.array of annotations of shape (N, 4) for (x1, y1, x2, y2).\n",
        "        annotations: np.array of shape (N, 5) for (x1, y1, x2, y2, label).\n",
        "        negative_overlap: IoU overlap for negative anchors (all anchors with overlap < negative_overlap are negative).\n",
        "        positive_overlap: IoU overlap or positive anchors (all anchors with overlap > positive_overlap are positive).\n",
        "\n",
        "    Returns\n",
        "        positive_indices: indices of positive anchors\n",
        "        ignore_indices: indices of ignored anchors\n",
        "        argmax_overlaps_inds: ordered overlaps indices\n",
        "    \"\"\"\n",
        "\n",
        "    overlaps = compute_overlap(anchors.astype(np.float64), annotations.astype(np.float64))   ### from keras_retinanet.utils.compute_overlap import compute_overlap\n",
        "    argmax_overlaps_inds = np.argmax(overlaps, axis=1)\n",
        "    max_overlaps = overlaps[np.arange(overlaps.shape[0]), argmax_overlaps_inds]\n",
        "\n",
        "    # assign \"dont care\" labels\n",
        "    positive_indices = max_overlaps >= positive_overlap\n",
        "    ignore_indices = (max_overlaps > negative_overlap) & ~positive_indices\n",
        "\n",
        "    return positive_indices, ignore_indices, argmax_overlaps_inds\n",
        "\n",
        "def bbox_transform(anchors, gt_boxes, mean=None, std=None):\n",
        "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
        "\n",
        "    if mean is None:\n",
        "        mean = np.array([0, 0, 0, 0])\n",
        "    if std is None:\n",
        "        std = np.array([0.2, 0.2, 0.2, 0.2])\n",
        "\n",
        "    if isinstance(mean, (list, tuple)):\n",
        "        mean = np.array(mean)\n",
        "    elif not isinstance(mean, np.ndarray):\n",
        "        raise ValueError('Expected mean to be a np.ndarray, list or tuple. Received: {}'.format(type(mean)))\n",
        "\n",
        "    if isinstance(std, (list, tuple)):\n",
        "        std = np.array(std)\n",
        "    elif not isinstance(std, np.ndarray):\n",
        "        raise ValueError('Expected std to be a np.ndarray, list or tuple. Received: {}'.format(type(std)))\n",
        "\n",
        "    anchor_widths  = anchors[:, 2] - anchors[:, 0]\n",
        "    anchor_heights = anchors[:, 3] - anchors[:, 1]\n",
        "\n",
        "    targets_dx1 = (gt_boxes[:, 0] - anchors[:, 0]) / anchor_widths\n",
        "    targets_dy1 = (gt_boxes[:, 1] - anchors[:, 1]) / anchor_heights\n",
        "    targets_dx2 = (gt_boxes[:, 2] - anchors[:, 2]) / anchor_widths\n",
        "    targets_dy2 = (gt_boxes[:, 3] - anchors[:, 3]) / anchor_heights\n",
        "\n",
        "    targets = np.stack((targets_dx1, targets_dy1, targets_dx2, targets_dy2))\n",
        "    targets = targets.T\n",
        "\n",
        "    targets = (targets - mean) / std\n",
        "\n",
        "    return targets\n",
        "\n",
        "\n",
        "def compute_input_output(image_group, annotations_group):\n",
        "    \"\"\" Compute inputs and target outputs for the network.\n",
        "    \"\"\"\n",
        "    # perform preprocessing steps\n",
        "    image_group, annotations_group = preprocess_group(image_group, annotations_group)\n",
        "\n",
        "    # compute network inputs\n",
        "    inputs = compute_inputs(image_group)\n",
        "\n",
        "    # compute network targets\n",
        "    targets = compute_targets(image_group, annotations_group)\n",
        "\n",
        "    return inputs, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8USDeZEvs-w",
        "colab_type": "text"
      },
      "source": [
        "###defs: preprocess, run dream, detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLE5Mf1vLhRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess_image, read_image_bgr, resize_image imported from keras_retinanet.utils.image\n",
        "\n",
        "def load_and_resize_image(path, min_side=800, max_side=1333):\n",
        "  in_img = read_image_bgr(path)\n",
        "  in_img, scale = resize_image(in_img, min_side=min_side, max_side=max_side)\n",
        "  return in_img, scale\n",
        "\n",
        "def deprocess_image(p_img, clip=True, touint8=True):\n",
        "  img = np.copy(p_img)\n",
        "  if mode == 'tf':\n",
        "    img += 1.\n",
        "    img *= 127.5\n",
        "  \n",
        "  elif mode == 'caffe':\n",
        "    imagenet_mean = np.array([103.939, 116.779, 123.68])\n",
        "    img += imagenet_mean\n",
        "  \n",
        "  if clip:\n",
        "    img = np.clip(img, 0, 255)\n",
        "\n",
        "  if touint8:\n",
        "    return np.uint8(img)\n",
        "  else:\n",
        "    return img\n",
        "\n",
        "def eval_loss_and_grads(x, fetch_loss_and_grads):\n",
        "    outs = fetch_loss_and_grads([x])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1]\n",
        "    return loss_value, grad_values\n",
        "\n",
        "def draw_caption_my(image, box, caption, color):\n",
        "    \"\"\" Draws a caption above the box in an image.\n",
        "\n",
        "    # Arguments\n",
        "        image   : The image to draw on.\n",
        "        box     : A list of 4 elements (x1, y1, x2, y2).\n",
        "        caption : String containing the text to draw.\n",
        "    \"\"\"\n",
        "    font = cv2.FONT_HERSHEY_PLAIN\n",
        "    font_scale = 1\n",
        "\n",
        "    (text_width, text_height) = cv2.getTextSize(caption, font, fontScale=font_scale, thickness=2)[0]\n",
        "    b = np.array(box).astype(int)\n",
        "    text_offset_x = b[0] + 1\n",
        "    text_offset_y = b[1] + text_height + 3\n",
        "    box_coords = ((text_offset_x-2, text_offset_y+2), (text_offset_x + text_width, text_offset_y - text_height - 3))\n",
        "    cv2.rectangle(image, box_coords[0], box_coords[1], color, cv2.FILLED)\n",
        "    cv2.putText(image, caption, (text_offset_x, text_offset_y), font, font_scale, (0, 0, 0), 2)\n",
        "    cv2.putText(image, caption, (text_offset_x, text_offset_y), font, font_scale, (255, 255, 255), 1)\n",
        "\n",
        "\n",
        "\n",
        "# detection on BGR image\n",
        "def detect(image, save=None, min_score=0.5, anns=None):\n",
        "  print(\"DETECTION, anns:\", anns)\n",
        "  \n",
        "  # copy to RGB draw on\n",
        "  draw = image.copy()\n",
        "  draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # preprocess BGR image for network\n",
        "  image = preprocess_image(image)\n",
        "\n",
        "  # process image\n",
        "  boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
        "\n",
        "  # draw original bb (white)\n",
        "  if anns is not None:\n",
        "    b = anns['bboxes'][0].astype(int)\n",
        "    cv2.rectangle(draw, (b[0], b[1]), (b[2], b[3]), (255,255,255), 1, cv2.LINE_AA)\n",
        "    draw_caption_my(draw, b, labels_to_names[anns['labels'][0]], (255,255,255))\n",
        "\n",
        "  # visualize detections\n",
        "  for box, score, label in reversed(list(zip(boxes[0], scores[0], labels[0]))):\n",
        "      # scores are sorted\n",
        "      if score < min_score:\n",
        "          continue\n",
        "          \n",
        "      color = label_color(label)\n",
        "      # draw box\n",
        "      b = box.astype(int)\n",
        "      draw_box(draw, b, color=color)\n",
        "      # draw caption\n",
        "      caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
        "      draw_caption_my(draw, b, caption, color)\n",
        "      \n",
        "  cv2_imshow(draw[:,:,::-1]) # covert draw from RGB to BGR\n",
        "\n",
        "  if save is not None:\n",
        "    cv2.imwrite('{}-detections.png'.format(save), draw[:,:,::-1])\n",
        "\n",
        "  return boxes[0], scores[0], labels[0]\n",
        "\n",
        "\n",
        "\n",
        "def dream_and_save(in_img, model, params, layer_dict=None,\n",
        "                   show=True, save=True, \n",
        "                   use_octaves=False):\n",
        "  \n",
        "  out_img_batch, out_loss, out_process_time, out_d_images = deep_dream(in_img, model, params, layer_dict=layer_dict)\n",
        "    \n",
        "  dtnow = datetime.now()\n",
        "  timestamp = dtnow.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "  \n",
        "  if params['layer_name'] is None:\n",
        "    result_prefix = 'RN_anns' + '_eta-' + str(params['eta']) + '_its-' + str(params['iterations']) + '_mode-' + mode\n",
        "  else:\n",
        "    result_prefix = 'RN_lr-' + params['layer_name'] + '_ch-' + str(params['channel_index']) + '_eta-' + str(params['eta']) + '_its-' + str(params['iterations']) + '_mode-' + mode\n",
        "\n",
        "  dep_img = deprocess_image(out_img_batch[0])\n",
        "\n",
        "  # save clean image (cv2 takes BGR)\n",
        "  cv2.imwrite('{}{}_{}_loss-{:.2f}_time-{:.2f}-clean.png'.format(save_path, timestamp, result_prefix, out_loss, out_process_time), dep_img)\n",
        "  \n",
        "  # plot and save plot image\n",
        "  plt.figure(figsize=fig_size)\n",
        "  print(fig_size)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(dep_img[:,:,::-1])     # to RGB pre zobrazenie\n",
        "  if params['layer_name'] is not None:\n",
        "    plt.title('{}\\nloss: {:.2f} time: {:.2f}'.format(result_prefix, out_loss, out_process_time))\n",
        "  else:\n",
        "    plt.title('{}\\nL-BFGS: {} loss: {:.2f} time: {:.2f}\\nlabels:{} bboxes:{}'.format(result_prefix, params['lbfgs'], out_loss, out_process_time, params['ann']['labels'], params['ann']['bboxes']))\n",
        "  plt.tight_layout()\n",
        "  if save:\n",
        "    plt.savefig('{}{}_{}_loss-{:.2f}_time-{:.2f}.png'.format(save_path, timestamp, result_prefix, out_loss, out_process_time))\n",
        "  if show:\n",
        "    plt.show()\n",
        "  \n",
        "  return dep_img, out_img_batch[0], '{}{}_{}_loss-{:.2f}_time-{:.2f}'.format(save_path, timestamp, result_prefix, out_loss, out_process_time), out_d_images\n",
        "\n",
        "# script to run multiple deams\n",
        "def run_dream(img, model, settings, show=True, save=True):\n",
        "  model_input = model.input\n",
        "  layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "  dreams = []\n",
        "\n",
        "  for i, layer_name in enumerate(settings['layers']):\n",
        "    for channel_index in settings['ch_indexes']:\n",
        "      if layer_name is None:\n",
        "        print('PROCESSING anns: {}'.format(settings['ann']))\n",
        "      else:\n",
        "        print('PROCESSING layer: {} channel: {}'.format(layer_name, channel_index))\n",
        "      params = {\n",
        "          'layer_name': layer_name,\n",
        "          'channel_index': channel_index,\n",
        "          'eta': settings['eta'],\n",
        "          'iterations': settings['iterations'],\n",
        "          'max_loss': settings['max_loss'][i],\n",
        "          'ann': settings['ann'],\n",
        "          'lbfgs': settings['lbfgs'],\n",
        "          'optim': settings['optim'],\n",
        "      }\n",
        "      dream = dream_and_save(np.copy(img), model, params, layer_dict, show=show, save=save, use_octaves=settings['use_octaves'])\n",
        "      dreams.append(dream)\n",
        "\n",
        "  return dreams\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ejnW5xvyl5",
        "colab_type": "text"
      },
      "source": [
        "###MyDeepDream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa0Tz2RQvxqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deep_dream(in_img, model, params, layer_dict=None):\n",
        "  start = time.time()\n",
        "\n",
        "  model_input = model.input\n",
        "  layer_name = params['layer_name']\n",
        "  channel_index = params['channel_index']\n",
        "  eta = params['eta']\n",
        "  iterations = params['iterations']\n",
        "  max_loss = params['max_loss']\n",
        "  lbfgs = params['lbfgs']\n",
        "  optim = params['optim']\n",
        "  if layer_dict == None:\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "  \n",
        "  if layer_name is not None:\n",
        "    if layer_name not in layer_dict:\n",
        "      raise ValueError('Layer ' + layer_name + ' not found in model.')\n",
        "    layer = layer_dict[layer_name].output\n",
        "    if layer.shape[3] < channel_index:\n",
        "      raise ValueError('Layer ' + layer_name + ' has only {} channels.'.format(layer.shape[3]))\n",
        "  \n",
        "  ann = params['ann']\n",
        "\n",
        "  inputs, targets = compute_input_output([in_img], [ann])\n",
        "  regression_batch, labels_batch = targets\n",
        "\n",
        "  img_batch = preprocess_image(np.copy(in_img))[np.newaxis, :, :, :]\n",
        "  print(img_batch.dtype)\n",
        "\n",
        "  # get layers\n",
        "  classification_layer = layer_dict['classification']\n",
        "  regression_layer = layer_dict['clipped_boxes']\n",
        "  detections = layer_dict['filtered_detections']\n",
        "\n",
        "  ##############################################################################\n",
        "  #  loss and grads\n",
        "  ##############################################################################\n",
        "\n",
        "  # uncommentx this for layer or channel axtivations maximization and comment out retinanet loss\n",
        "  # loss = K.mean(K.square(layer if channel_index is None else layer[:,:,:,channel_index]))\n",
        "\n",
        "\n",
        "  # RetinaNet loss\n",
        "  focal_loss = focal()\n",
        "  smooth_L1_loss = smooth_l1()\n",
        "  loss = focal_loss(labels_batch, classification_layer.output) + smooth_L1_loss(regression_batch, regression_layer.output)\n",
        "\n",
        "\n",
        "  # gradients\n",
        "  grads = K.gradients(loss, model_input)[0]\n",
        "\n",
        "  fetch_loss_and_grads = K.function([model_input], [loss, grads, detections.output])\n",
        "\n",
        "  d_images = []\n",
        "  \n",
        "  ##############################################################################\n",
        "  #  LBFGS\n",
        "  ##############################################################################\n",
        "  if lbfgs:\n",
        "\n",
        "    def fun(x):\n",
        "      global lbfgs_iter\n",
        "      x = np.reshape(x, img_batch.shape)\n",
        "      outs = fetch_loss_and_grads([x.astype(np.float32)])\n",
        "      print(lbfgs_iter, outs[0])\n",
        "      lbfgs_iter += 1\n",
        "      return outs[0].astype(np.float64), np.ravel(outs[1]).astype(np.float64)\n",
        "    \n",
        "    outs = fetch_loss_and_grads([img_batch])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1]\n",
        "    boxes = outs[2][0][0]\n",
        "    scores = outs[2][1][0]\n",
        "    labels = outs[2][2][0]\n",
        "    detections = np.array(list(zip(scores, labels, boxes)))\n",
        "    print(detections[:3])\n",
        "\n",
        "    from scipy.optimize import minimize\n",
        "\n",
        "    for i in range(iterations):\n",
        "      res = minimize(fun, img_batch.astype(np.float64), method='L-BFGS-B', jac=True, options={'maxiter': 1000, 'disp': True, 'iprint':100} )\n",
        "      print(res)\n",
        "      img_batch = np.reshape(res.x.astype(np.float32), img_batch.shape)\n",
        "      outs = fetch_loss_and_grads([img_batch])\n",
        "      loss_value = outs[0]\n",
        "      grad_values = outs[1]\n",
        "      boxes = outs[2][0][0]\n",
        "      scores = outs[2][1][0]\n",
        "      labels = outs[2][2][0]\n",
        "      detections = np.array(list(zip(scores, labels, boxes)))\n",
        "      print(detections[:3])\n",
        "\n",
        "      d_img = deprocess_image(img_batch[0])\n",
        "      d_images.append((d_img, i))\n",
        "\n",
        "      cv2_imshow(d_img)\n",
        "\n",
        "      cv2_imshow(10 * np.abs(d_img.astype(np.float32) - in_img.astype(np.float32)).astype(np.uint8))\n",
        "\n",
        "      img_batch = preprocess_image(d_img)[np.newaxis,:,:,:]\n",
        "\n",
        "    process_time = time.time() - start\n",
        "    print('Process time: {:.2f}'.format(process_time))\n",
        "\n",
        "    d_images.append((deprocess_image(img_batch[0]),0))\n",
        "    return img_batch, loss_value, process_time, d_images\n",
        "  \n",
        "\n",
        "  ##############################################################################\n",
        "  #  gradient ascent/descent\n",
        "  ##############################################################################\n",
        "  \n",
        "  for i in range(iterations):\n",
        "    step_start = time.time()\n",
        "\n",
        "    print()\n",
        "\n",
        "    # fetch loss, grads and outs\n",
        "    outs = fetch_loss_and_grads([img_batch])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1]\n",
        "    boxes = outs[2][0][0]\n",
        "    scores = outs[2][1][0]\n",
        "    labels = outs[2][2][0]\n",
        "\n",
        "    old_img = deprocess_image(np.copy(img_batch[0]))\n",
        "    \n",
        "    print(\"Max grad val {}\".format(np.max(np.abs(grad_values))))\n",
        "    print(grad_values.shape)\n",
        "\n",
        "    # update image wrt. gradients\n",
        "    if eta is None:\n",
        "      img_delta = 0.1 * grad_values / np.max(np.abs(grad_values)) \n",
        "    elif eta > 0:\n",
        "      img_delta = eta * grad_values\n",
        "    else:\n",
        "      img_delta = np.sign(grad_values)\n",
        "    \n",
        "    img_batch += optim * img_delta       # optim = 1 for maximization, -1 for minimization\n",
        "\n",
        "    # print out detections\n",
        "    detections = np.array(list(zip(scores, labels, boxes)))\n",
        "    print(detections[:3])\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      d_img = deprocess_image(img_batch[0])\n",
        "      d_images.append((d_img, i))\n",
        "    \n",
        "    # deprocess normalization\n",
        "    if i % 100 == 0:\n",
        "      d_img = deprocess_image(img_batch[0])\n",
        "      cv2_imshow(d_img)\n",
        "\n",
        "      print(\"Diff img\")        \n",
        "      print(np.max(np.abs(d_img.astype(np.float32) - in_img.astype(np.float32))))\n",
        "      cv2_imshow((10 * np.abs(d_img.astype(np.float32) - in_img.astype(np.float32)).astype(np.uint8))[:,:,::-1])\n",
        "      \n",
        "      img_batch = preprocess_image(d_img)[np.newaxis,:,:,:]\n",
        "\n",
        "    print(\"At step: {:3d}     loss:{:7.2f}     step_time:{:5.2f}\".format(i, loss_value, time.time() - step_start))\n",
        "\n",
        "    if max_loss is not None and loss_value > max_loss:\n",
        "      break\n",
        "\n",
        "  d_img = deprocess_image(img_batch[0])\n",
        "  d_images.append((d_img, i))\n",
        "  \n",
        "  process_time = time.time() - start\n",
        "  print('Process time: {:.2f}'.format(process_time))\n",
        "\n",
        "  return img_batch, loss_value, process_time, d_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td3pHU3mkwJa",
        "colab_type": "text"
      },
      "source": [
        "###Min attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k3RcWYooqlSD",
        "colab": {}
      },
      "source": [
        "def min_max_attack(in_img, model, params, layer_dict=None):\n",
        "  start = time.time()\n",
        "\n",
        "  model_input = model.input\n",
        "  iterations = params['iterations']\n",
        "  if layer_dict == None:\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "\n",
        "  ann = params['ann']\n",
        "\n",
        "  inputs, targets = compute_input_output([in_img], [ann])\n",
        "  regression_batch, labels_batch = targets\n",
        "\n",
        "  img_batch = preprocess_image(np.copy(in_img))[np.newaxis, :, :, :]\n",
        "\n",
        "  # get layers  \n",
        "  classification_layer = layer_dict['classification']\n",
        "  regression_layer = layer_dict['clipped_boxes']\n",
        "  detections = layer_dict['filtered_detections']\n",
        "  \n",
        "  # RetinaNet loss\n",
        "  focal_loss = focal()\n",
        "  smooth_L1_loss = smooth_l1()\n",
        "  loss = focal_loss(labels_batch, classification_layer.output) + smooth_L1_loss(regression_batch, regression_layer.output)\n",
        "\n",
        "\n",
        "  # gradients\n",
        "  grads = K.gradients(loss, model_input)[0]\n",
        "\n",
        "  fetch_loss_and_grads = K.function([model_input], [loss, grads, detections.output])\n",
        "\n",
        "  for i in range(iterations):\n",
        "    # fetch loss, grads and outs\n",
        "    outs = fetch_loss_and_grads([img_batch])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1]\n",
        "    boxes = outs[2][0][0]\n",
        "    scores = outs[2][1][0]\n",
        "    labels = outs[2][2][0]\n",
        "    \n",
        "    print(\"Max grad val {}\".format(np.max(np.abs(grad_values))))\n",
        "    print(grad_values.shape)\n",
        "\n",
        "    n = params['px']\n",
        "    rgrads = np.abs(np.ravel(grad_values))\n",
        "    arm = np.argsort(rgrads)\n",
        "    for j in range(n):\n",
        "      idx = np.unravel_index(arm[-j], grad_values.shape)\n",
        "      # print('idx', j, idx)\n",
        "      img_batch[idx] -= 255 * np.sign(grad_values[idx])\n",
        "      if j % params['print'] == 0:\n",
        "        dtnow = datetime.now()\n",
        "        timestamp = dtnow.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        savename = params['save_path'] + timestamp + '_' + str(n) + '_' + str(j)\n",
        "        d_img = deprocess_image(img_batch[0])\n",
        "        plt.figure(figsize=(10,10))\n",
        "        diff = (10 * np.abs(d_img.astype(np.float32) - in_img.astype(np.float32)).astype(np.uint8))\n",
        "        plt.imshow(diff[:,:,::-1])\n",
        "        plt.show()      \n",
        "        cv2.imwrite('{}-diff.png'.format(savename), diff)\n",
        "        cv2.imwrite('{}-clean.png'.format(savename), d_img)\n",
        "        detects = detect(d_img, save=savename)\n",
        "\n",
        "    d_img = deprocess_image(img_batch[0])\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow((10 * np.abs(d_img.astype(np.float32) - in_img.astype(np.float32)).astype(np.uint8))[:,:,::-1])\n",
        "    plt.show()\n",
        "\n",
        "  return img_batch, loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFPkWEoEktPJ",
        "colab_type": "text"
      },
      "source": [
        "###Adversarial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEttFa99i0c7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adversarial(in_img, model, params, layer_dict=None):\n",
        "  model_input = model.input\n",
        "  iterations = params['iterations']\n",
        "  optim = params['optim']\n",
        "  if layer_dict == None:\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "  \n",
        "  ann = params['ann']\n",
        "\n",
        "  inputs, targets = compute_input_output([in_img], [ann])\n",
        "  regression_batch, labels_batch = targets\n",
        "\n",
        "  img_batch = preprocess_image(np.copy(in_img))[np.newaxis, :, :, :]\n",
        "\n",
        "  # get layers\n",
        "  classification_layer = layer_dict['classification']\n",
        "  regression_layer = layer_dict['clipped_boxes']\n",
        "  detections = layer_dict['filtered_detections']\n",
        "\n",
        "  ##############################################################################\n",
        "  #  loss and grads\n",
        "  ##############################################################################\n",
        "\n",
        "  # RetinaNet loss\n",
        "  focal_loss = focal()\n",
        "  smooth_L1_loss = smooth_l1()\n",
        "  loss = focal_loss(labels_batch, classification_layer.output) + smooth_L1_loss(regression_batch, regression_layer.output)\n",
        "\n",
        "  # gradients\n",
        "  grads = K.gradients(loss, model_input)[0]\n",
        "\n",
        "  fetch_loss_and_grads = K.function([model_input], [loss, grads, detections.output])\n",
        "\n",
        "  d_images = []\n",
        "  \n",
        "  ##############################################################################\n",
        "  #  gradient ascent/descent\n",
        "  ##############################################################################\n",
        "  \n",
        "  for i in range(iterations):\n",
        "    print()\n",
        "\n",
        "    # fetch loss, grads and outs\n",
        "    outs = fetch_loss_and_grads([img_batch])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1]\n",
        "    boxes = outs[2][0][0]\n",
        "    scores = outs[2][1][0]\n",
        "    labels = outs[2][2][0]\n",
        "\n",
        "    old_img = deprocess_image(np.copy(img_batch[0]))\n",
        "    \n",
        "    print(\"Max grad val {}\".format(np.max(np.abs(grad_values))))\n",
        "    print(grad_values.shape)\n",
        "\n",
        "    # update image wrt. gradients\n",
        "    img_delta = np.sign(grad_values)\n",
        "    img_batch += optim * img_delta       # optim = 1 for maximization, -1 for minimization\n",
        "    \n",
        "    # print out detections\n",
        "    detections = np.array(list(zip(scores, labels, boxes)))\n",
        "    print(detections[:3])\n",
        "\n",
        "    if i % params['print'] == 0:\n",
        "\n",
        "      timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "      savename = params['save_path'] + timestamp + '_adv_' + str(i)\n",
        "      d_img = deprocess_image(img_batch[0])\n",
        "      d_images.append((d_img, i))\n",
        "\n",
        "      diff = (10 * np.abs(d_img.astype(np.float32) - in_img.astype(np.float32)).astype(np.uint8))\n",
        "      plt.figure(figsize=(10,10))\n",
        "      plt.imshow(diff[:,:,::-1])\n",
        "      plt.show()      \n",
        "      cv2.imwrite('{}-diff.png'.format(savename), diff)\n",
        "      detects = detect(d_img, save=savename)\n",
        "\n",
        "    # deprocess normalization\n",
        "    if i % 100 == 0:\n",
        "      plt.imshow(d_img[:,:,::-1])     #### RGB len pre zobrazenie\n",
        "      plt.show()\n",
        "\n",
        "      print(\"Diff img\")         \n",
        "      print(np.max(np.abs(d_img.astype(np.float32) - in_img.astype(np.float32))))\n",
        "\n",
        "      plt.imshow((10 * np.abs(d_img.astype(np.float32) - in_img.astype(np.float32)).astype(np.uint8))[:,:,::-1])\n",
        "      plt.show()\n",
        "      \n",
        "      img_batch = preprocess_image(d_img)[np.newaxis,:,:,:]\n",
        "\n",
        "    print(\"At step: {:3d}     loss:{:7.2f}\".format(i, loss_value))\n",
        "\n",
        "  d_img = deprocess_image(img_batch[0])\n",
        "  d_images.append((d_img, i))\n",
        "  \n",
        "\n",
        "  return img_batch, loss_value, d_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHwk8M6n0mkW",
        "colab_type": "text"
      },
      "source": [
        "###DeepDream attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRsrT2wP0pJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deep_dream_attack(in_img, model, params, layer_dict=None):\n",
        "  model_input = model.input\n",
        "  eta = params['eta']\n",
        "  iterations = params['iterations']\n",
        "  lbfgs = params['lbfgs']\n",
        "  optim = params['optim']\n",
        "  if layer_dict == None:\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "  \n",
        "  ann = params['ann']\n",
        "\n",
        "  inputs, targets = compute_input_output([in_img], [ann])\n",
        "  regression_batch, labels_batch = targets\n",
        "\n",
        "  img_batch = preprocess_image(np.copy(in_img))[np.newaxis, :, :, :]\n",
        "\n",
        "  # get layers\n",
        "  classification_layer = layer_dict['classification']\n",
        "  regression_layer = layer_dict['clipped_boxes']\n",
        "  detections = layer_dict['filtered_detections']\n",
        "\n",
        "  ##############################################################################\n",
        "  #  loss and grads\n",
        "  ##############################################################################\n",
        "\n",
        "  # RetinaNet loss\n",
        "  focal_loss = focal()\n",
        "  smooth_L1_loss = smooth_l1()\n",
        "  loss = focal_loss(labels_batch, classification_layer.output) + smooth_L1_loss(regression_batch, regression_layer.output)\n",
        "\n",
        "  # gradients\n",
        "  grads = K.gradients(loss, model_input)[0]\n",
        "\n",
        "  fetch_loss_and_grads = K.function([model_input], [loss, grads, detections.output])\n",
        "\n",
        "  d_images = []\n",
        "  \n",
        "  ##############################################################################\n",
        "  #  LBFGS\n",
        "  ##############################################################################\n",
        "  if lbfgs:\n",
        "\n",
        "    def fun(x):\n",
        "      global lbfgs_iter\n",
        "      x = np.reshape(x, img_batch.shape)\n",
        "      outs = fetch_loss_and_grads([x.astype(np.float32)])\n",
        "      print(lbfgs_iter, outs[0])\n",
        "      lbfgs_iter += 1\n",
        "      return outs[0].astype(np.float64), np.ravel(outs[1]).astype(np.float64)\n",
        "    \n",
        "    outs = fetch_loss_and_grads([img_batch])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1]\n",
        "    boxes = outs[2][0][0]\n",
        "    scores = outs[2][1][0]\n",
        "    labels = outs[2][2][0]\n",
        "    detections = np.array(list(zip(scores, labels, boxes)))\n",
        "    print(detections[:3])\n",
        "\n",
        "    from scipy.optimize import minimize\n",
        "\n",
        "    for i in range(iterations):\n",
        "      res = minimize(fun, img_batch.astype(np.float64), method='L-BFGS-B', jac=True, options={'maxiter': 1000, 'disp': True, 'iprint':100} )\n",
        "      print(res)\n",
        "      img_batch = np.reshape(res.x.astype(np.float32), img_batch.shape)\n",
        "      outs = fetch_loss_and_grads([img_batch])\n",
        "      loss_value = outs[0]\n",
        "      grad_values = outs[1]\n",
        "      boxes = outs[2][0][0]\n",
        "      scores = outs[2][1][0]\n",
        "      labels = outs[2][2][0]\n",
        "      detections = np.array(list(zip(scores, labels, boxes)))\n",
        "      print(detections[:3])\n",
        "\n",
        "      d_img = deprocess_image(img_batch[0])\n",
        "      d_images.append((d_img, i))\n",
        "\n",
        "      plt.imshow(d_img[:,:,::-1])     #### RGB len pre zobrazenie\n",
        "      plt.show()\n",
        "\n",
        "      plt.imshow(10 * np.abs(d_img.astype(np.float32) - in_img.astype(np.float32)).astype(np.uint8))\n",
        "      plt.show()\n",
        "\n",
        "      img_batch = preprocess_image(d_img)[np.newaxis,:,:,:]\n",
        "\n",
        "    d_images.append((deprocess_image(img_batch[0]),0))\n",
        "    return img_batch, loss_value, d_images\n",
        "  \n",
        "\n",
        "  ##############################################################################\n",
        "  #  gradient ascent/descent\n",
        "  ##############################################################################\n",
        "  \n",
        "  for i in range(iterations):\n",
        "    print()\n",
        "\n",
        "    # fetch loss, grads and outs\n",
        "    outs = fetch_loss_and_grads([img_batch])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1]\n",
        "    boxes = outs[2][0][0]\n",
        "    scores = outs[2][1][0]\n",
        "    labels = outs[2][2][0]    \n",
        "\n",
        "    # update image wrt. gradients\n",
        "    if eta is None:\n",
        "      img_delta = 0.1 * grad_values / np.max(np.abs(grad_values)) \n",
        "    elif eta > 0:\n",
        "      img_delta = eta * grad_values\n",
        "    else:\n",
        "      img_delta = np.sign(grad_values)\n",
        "    \n",
        "    img_batch += optim * img_delta       # optim = 1 for maximization, -1 for minimization\n",
        "    \n",
        "    # print out detections\n",
        "    detections = np.array(list(zip(scores, labels, boxes)))\n",
        "    print(detections[:3])\n",
        "\n",
        "    if i % params['print'] == 0:\n",
        "      timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "      savename = params['save_path'] + timestamp + '_dda_' + str(i)\n",
        "      d_img = deprocess_image(img_batch[0])\n",
        "      d_images.append((d_img, i))\n",
        "\n",
        "      cv2_imshow(d_img)\n",
        "      cv2.imwrite('{}-d_img.png'.format(savename), d_img)\n",
        "\n",
        "      diff = (10 * np.abs(d_img.astype(np.float32) - in_img.astype(np.float32)).astype(np.uint8))\n",
        "\n",
        "      cv2_imshow(diff)     \n",
        "      cv2.imwrite('{}-diff.png'.format(savename), diff)\n",
        "      detects = detect(d_img, save=savename)\n",
        "\n",
        "    \n",
        "    # deprocess normalization\n",
        "    if i % params['dep_norm'] == 0:\n",
        "      d_img = deprocess_image(img_batch[0])\n",
        "\n",
        "      img_batch = preprocess_image(d_img)[np.newaxis,:,:,:]\n",
        "    \n",
        "    print(\"At step: {:3d}     loss:{:7.2f}\".format(i, loss_value))\n",
        "\n",
        "  d_img = deprocess_image(img_batch[0])\n",
        "  d_images.append((d_img, i))\n",
        "  \n",
        "  return d_img, loss_value, d_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCsFQ_alAmA3",
        "colab_type": "text"
      },
      "source": [
        "###Settings  DeepDream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU7C5SiYOLti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lbfgs_iter = 0 # ugly method how to show iter number in lbfgs\n",
        "settings = {\n",
        "  'layers': [None],     # put 'P3', 'P4', ... , 'P7' to analyse layer\n",
        "  'max_loss': [None],   # for each layer sepparately\n",
        "  'ch_indexes': [None], # choose channels or leave None for analisis of the whole layer\n",
        "  'eta': 1000,          # None for automatic eta / 0 for sign gradient optimization / >0 for constant eta\n",
        "  'iterations': 100,\n",
        "  'use_octaves': False,\n",
        "  'lbfgs': False,\n",
        "  'optim': -1,          # 1 for maximization / -1 for minimization\n",
        "\n",
        "  'ann': {              # at least one annotation shold be there, even if it is not used\n",
        "    'labels': np.array([14]), # bird\n",
        "    'bboxes': np.array([\n",
        "                    np.array([250.0, 40.0, 350.0, 110.0])\n",
        "              ])\n",
        "    },\n",
        "  # 'ann': {\n",
        "  #   'labels': np.array([0]), # person\n",
        "  #   'bboxes': np.array([\n",
        "  #                   np.array([180.0, 150.0, 320.0, 480.0])\n",
        "  #             ])\n",
        "  #   },\n",
        "  # 'ann': {\n",
        "  #   'labels': np.array([4]), # plane\n",
        "  #   'bboxes': np.array([\n",
        "  #                   np.array([50.0, 80.0, 150.0, 150.0])\n",
        "  #             ])\n",
        "  #   },\n",
        "  # 'ann': {\n",
        "  #   'labels': np.array([20]), # elephant\n",
        "  #   'bboxes': np.array([\n",
        "  #                   np.array([100.0, 200.0, 400.0, 450.0])\n",
        "  #             ])\n",
        "  #   },\n",
        "  # 'ann': {\n",
        "  #   'labels': np.array([47]), # aplle\n",
        "  #   'bboxes': np.array([\n",
        "  #                   np.array([80.0, 360.0, 150.0, 430.0])\n",
        "  #             ])\n",
        "  #   },\n",
        "}\n",
        "\n",
        "\n",
        "path = drive_path + 'images/dogs.jpg'  # replace by your desired images\n",
        "img = read_image_bgr(path)\n",
        "\n",
        "# seda\n",
        "# img = 127 * np.ones([500,500,3], dtype=np.uint8)\n",
        "\n",
        "# random uniform noise\n",
        "# img = np.random.uniform(size=[500,500,3], low=0.0, high=255.0).astype(np.uint8)\n",
        "\n",
        "# cierna\n",
        "#img = np.zeros([600,600,3], dtype=np.uint8)\n",
        "\n",
        "# biela \n",
        "#img = 255 * np.ones([600,600,3], dtype=np.uint8)\n",
        "\n",
        "\n",
        "# measure total process time\n",
        "start_time = time.time()\n",
        "\n",
        "# run deepdream\n",
        "dreams = run_dream(img, model, settings, show=True, save=True)\n",
        "# boxes, scores, labels = detect(dreams[0][0], save=dreams[0][2], min_score=0.5, anns=settings['ann']) # uncomment to run detection\n",
        "\n",
        "total_process_time = time.time() - start_time\n",
        "print('TOTAL process time: {:.2f}'.format(total_process_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGE1VLPpj9T2",
        "colab_type": "text"
      },
      "source": [
        "###Settings2  DeepDream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGyPqfi9TC6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# just another cells for deepdreaming\n",
        "\n",
        "path = drive_path + 'images/dog.jpg'\n",
        "img = read_image_bgr(path)\n",
        "\n",
        "boxes, scores, labels = detect(img)\n",
        "\n",
        "# synthetized annotations\n",
        "_labels, _bboxes = [], []\n",
        "for box, score, label in zip(boxes, scores, labels):\n",
        "  if score < 0.5:\n",
        "    break\n",
        "  _labels.append(label)\n",
        "  _bboxes.append(box)\n",
        "\n",
        "digital_anns = {\n",
        "  'labels': np.array(_labels),\n",
        "  'bboxes': np.array(_bboxes)\n",
        "}\n",
        "digital_anns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiHeY-o2LDTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(digital_anns)\n",
        "lbfgs_iter = 0 # ugly way how to show iter number in lbfgs\n",
        "settings2 = {\n",
        "  'layers': [None],\n",
        "  'max_loss': [None], \n",
        "  'ch_indexes': [None],\n",
        "  'eta': None,  # None for automatic eta / 0 for sign gradient optimization / >0 for constant eta\n",
        "  'iterations': 1000,\n",
        "  'use_octaves': False,\n",
        "  'ann': digital_anns,\n",
        "  'lbfgs': False,\n",
        "  'optim': 1,\n",
        "}\n",
        "\n",
        "dreams = run_dream(np.copy(img), model, settings2, show=True, save=True)\n",
        "boxes, scores, labels = detect(dreams[0][0], save=dreams[0][2], min_score=0.5)\n",
        "print(np.array(list(zip(scores, labels, boxes)))[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoZU8BBtkAcU",
        "colab_type": "text"
      },
      "source": [
        "###Settings3  Minimal attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Ruwd1kt4jZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = drive_path + 'images/stop.jpg'\n",
        "img = read_image_bgr(path)\n",
        "\n",
        "boxes, scores, labels = detect(img)\n",
        "\n",
        "# synthetized annotations\n",
        "_labels, _bboxes = [], []\n",
        "for box, score, label in zip(boxes, scores, labels):\n",
        "  if score < 0.5:\n",
        "    break\n",
        "  _labels.append(label)\n",
        "  _bboxes.append(box)\n",
        "\n",
        "digital_anns = {\n",
        "  'labels': np.array(_labels),\n",
        "  'bboxes': np.array(_bboxes)\n",
        "}\n",
        "digital_anns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "burv0wNisojo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "settings3 = {\n",
        "  'iterations': 1,\n",
        "  'px': 100000,\n",
        "  'ann': digital_anns,\n",
        "  'save_path': save_path,\n",
        "  'print': 10000,\n",
        "}\n",
        "\n",
        "dtnow = datetime.now()\n",
        "timestamp = dtnow.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "savename = save_path + timestamp + str(settings3['px']) + str(digital_anns)\n",
        "\n",
        "attack = min_max_attack(np.copy(img), model, settings3)\n",
        "img_res = deprocess_image(attack[0][0])\n",
        "dets = detect(img_res, save=savename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaHo1GY3kNyH",
        "colab_type": "text"
      },
      "source": [
        "###Settings4  Adversarial attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVVtAoLpkZb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = drive_path + 'images/stop.jpg'\n",
        "img = read_image_bgr(path)\n",
        "\n",
        "boxes, scores, labels = detect(img)\n",
        "\n",
        "_labels, _bboxes = [], []\n",
        "\n",
        "for box, score, label in zip(boxes, scores, labels):\n",
        "  if score < 0.5:\n",
        "    break\n",
        "  if label == 0:\n",
        "    _labels.append(label)\n",
        "    _bboxes.append(box)\n",
        "\n",
        "# _labels, _bboxes = [], [] # uncomment this if we want empty annotations\n",
        "\n",
        "digital_anns = {\n",
        "  'labels': np.array(_labels),\n",
        "  'bboxes': np.array(_bboxes)\n",
        "}\n",
        "digital_anns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r__n2mWJkM02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "settings4 = {\n",
        "  'iterations': 5,\n",
        "  'optim': -1,\n",
        "  'ann': digital_anns,\n",
        "  'save_path': save_path,\n",
        "  'print': 1,\n",
        "}\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "savename = save_path + timestamp + str(settings4['iterations'])\n",
        "\n",
        "attack = adversarial(np.copy(img), model, settings4)\n",
        "img_res = deprocess_image(attack[0][0])\n",
        "dets = detect(img_res, save=savename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzgVncwcyPeQ",
        "colab_type": "text"
      },
      "source": [
        "###Settings5  DeepDream attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV70f_d1yeG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = drive_path + 'images/stop.jpg'\n",
        "img = read_image_bgr(path)\n",
        "\n",
        "boxes, scores, labels = detect(img)\n",
        "_labels, _bboxes = [], []\n",
        "for box, score, label in zip(boxes, scores, labels):\n",
        "  if score < 0.5:\n",
        "    break\n",
        "  _labels.append(label)\n",
        "  _bboxes.append(box)\n",
        "\n",
        "# _labels, _bboxes = [], [] # uncomment this if we want empty annotations\n",
        "\n",
        "digital_anns = {\n",
        "  'labels': np.array(_labels),\n",
        "  'bboxes': np.array(_bboxes)\n",
        "}\n",
        "digital_anns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2e3O4nSyTXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lbfgs_iter = 0 # ugly method how to show iter number in lbfgs\n",
        "\n",
        "settings5 = { \n",
        "  'layers': [None],\n",
        "  'max_loss': [None],\n",
        "  'ch_indexes': [None], \n",
        "  'eta': 1000, # None for automatic eta / 0 for sign gradient optimization / >0 for constant eta\n",
        "  'iterations': 10,\n",
        "  'use_octaves': False,\n",
        "  'lbfgs': False,\n",
        "  'optim': -1,\n",
        "  'dep_norm': 100,\n",
        "  'print': 1,\n",
        "  'save_path': save_path,\n",
        "  'ann': digital_anns,\n",
        "\n",
        "}\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "savename = save_path + timestamp + str(settings5['iterations'])\n",
        "\n",
        "# run deepdream\n",
        "dreams = deep_dream_attack(img, model, settings5)    # 1.[] - dream, 2.[0] - result image / [2] - savepath \n",
        "boxes, scores, labels = detect(dreams[0], save=savename, min_score=0.5, anns=None)\n",
        "print(np.array(list(zip(scores, labels, boxes)))[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}